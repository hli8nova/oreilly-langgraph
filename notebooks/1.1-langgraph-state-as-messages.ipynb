{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State as Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to keep a log of the conversation? Something quite common in LLM applications.\n",
    "How could we do that across a traditional langgraph graph?\n",
    "\n",
    "The answer is by defining a state that allows us to add messages as the graph gets executed!\n",
    "\n",
    "But wait, what are messages?\n",
    "\n",
    "## Messages\n",
    "\n",
    "[Messages](https://python.langchain.com/v0.2/docs/concepts/#messages), capture different roles within a conversation. \n",
    "\n",
    "LangChain supports various message types, including \n",
    "\n",
    "1. `HumanMessage` - message from the user \n",
    "2. `AIMessage` - message from the chat model\n",
    "3. `SystemMessage` - message that instructs the behavior of the chat model\n",
    "4. `ToolMessage` - message from a tool call (calling a tool that performs some action) \n",
    "\n",
    "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call. \n",
    "\n",
    "Each message can be supplied with a few things:\n",
    "\n",
    "* `content` - content of the message\n",
    "* `name` - optionally, a message author \n",
    "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install openai==1.55.3 httpx==0.27.2 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "#load env variables from .env file\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(openai_key == None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxTG7Kk2xZhm",
    "outputId": "e08810b4-d44d-4fb6-d3e7-6dad5c406ad8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You write engaging Linkedin Posts.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=llm_name)\n",
    "\n",
    "generate = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåü Excited to Share My Experience at IEEE ICNC 2025! üåü\n",
      "\n",
      "On February 19, 2025, I had the incredible opportunity to present my research on **\"Graph Density Mutation Study to Improve Cyber-Physical Systems Intrusion Detection with Heterogeneous GNN and Express Edges\"** at the 2025 International Conference on Computing, Networking and Communications (ICNC) in beautiful Honolulu, Hawaii! \n",
      "\n",
      "This year's conference brought together brilliant minds and innovative thinkers from across the globe, and I was thrilled to discuss how leveraging heterogeneous Graph Neural Networks (GNN) and express edges can significantly enhance the detection of intrusions in complex cyber-physical systems. üåêüîí\n",
      "\n",
      "Through my presentation, I explored the critical role of graph density mutation in refining data representation and improving our models' accuracy and efficiency. The insights gained from this study have the potential to revolutionize how we approach cybersecurity in interconnected environments.\n",
      "\n",
      "A huge thank you to the IEEE community for hosting such an inspiring event and to everyone who engaged with my presentation. The discussions and feedback were invaluable! üôå\n",
      "\n",
      "If you're interested in the intersection of GNN, cybersecurity, and advancing technology, let's connect! Together, we can explore innovations that shape the future of our digital landscape. \n",
      "\n",
      "#IEEEICNC2025 #Cybersecurity #GraphNeuralNetworks #IntrusionDetection #ResearchPresentation #Networking #Innovation #Honolulu2025 #TechForGood\n"
     ]
    }
   ],
   "source": [
    "request = \"Write one about my presentation at IEEE ICNC2025 on 2/19/2025 with topic: Graph Density Mutation Study to Improve Cyber-Physical Systems Intrusion Detection with Heterogeneous GNN and Express Edges. 2025 International Conference on Computing, Networking and Communications (ICNC) took place February 17-20, 2025 in Honolulu, Hawaii, USA\"\n",
    "first_draft_response = generate.invoke({\"messages\": [(\"user\", request)]})\n",
    "print(first_draft_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So, you work in AI engineering?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lucas\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn about.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lucas\n",
      "\n",
      "I want to learn about fine tunning local LLMs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f\"So, you work in AI engineering?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lucas\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about fine tunning local LLMs.\", name=\"Lucas\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, messages store the exchanges between model and user throughout a conversation. We can have them as keys in a state for a graph\n",
    "so that we can keep a log of the interactions throughout the graph and so that the chat models within a graph can have the context information\n",
    "for what happened before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using messages as state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about what do we need to have these messages as parts of a graph state?\n",
    "We need:\n",
    "1. Something that stores the messages (like a list)\n",
    "2. Something that allows us to add more messages into this list as the graph is executed\n",
    "3. Something that can log metadata about the objects being added to the conversation so we can inspect them during debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='message 2', additional_kwargs={}, response_metadata={}, name='Lucas')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def my_node(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 1\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "def my_node2(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 2\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add the node\n",
    "workflow.add_node(\"test_node\", my_node)\n",
    "workflow.add_node(\"test_node2\", my_node2)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"test_node\")\n",
    "\n",
    "# Set the exit point\n",
    "workflow.add_edge(\"test_node\", \"test_node2\")\n",
    "workflow.add_edge(\"test_node2\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Run the graph with initial state\n",
    "result = graph.invoke({\n",
    "    \"messages\": messages  # Using messages defined above\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait! Why do we only have one message in the final output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducers\n",
    "\n",
    "In langgraph the default is to [override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior `messages` value.\n",
    " \n",
    "However what we want is to append messages to our `messages` state key throughout the graph's execution.\n",
    " \n",
    "For that we use something called [`reducer` functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers), which allow us to specify\n",
    "how the states get updated. To do that we annotate the `messages` key with the `add_messages` reducer function as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we take the same graph as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='message 1', additional_kwargs={}, response_metadata={}, name='Lucas', id='76414573-6bd8-4bab-a978-2ce8be8796f9'),\n",
       "  HumanMessage(content='message 2', additional_kwargs={}, response_metadata={}, name='Lucas', id='d7d1c9fe-151b-4ba7-835c-9abc2d69296d')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_node(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 1\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "def my_node2(state):\n",
    "    # Get current messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Add a new message\n",
    "    new_message = HumanMessage(content=\"message 2\", name=\"Lucas\")\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add the node\n",
    "workflow.add_node(\"test_node\", my_node)\n",
    "workflow.add_node(\"test_node2\", my_node2)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"test_node\")\n",
    "\n",
    "# Set the exit point\n",
    "workflow.add_edge(\"test_node\", \"test_node2\")\n",
    "workflow.add_edge(\"test_node2\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Run the graph with initial state\n",
    "result = graph.invoke({\n",
    "    \"messages\": []  # Using messages defined above\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that now both messages are stored inside after the graph's execution.\n",
    "\n",
    "LangGraph allows us to simplify this a bit so we don't have to be writing this overcomplicated-looking class everytime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# import openai\n",
    "\n",
    "# llm = ChatOpenAI(model=llm_name, openai_api_key=openai_key, http_client=None)\n",
    "# llm = ChatOpenAI(model=llm_name, openai_api_key=openai_key) #, http_client=None)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see a simple example actually using LLMs + messages as state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Tell me a short joke', additional_kwargs={}, response_metadata={}, id='e20280d3-3599-41fa-8a68-0dbfe1fe128b'),\n",
       "  AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 12, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'stop', 'logprobs': None}, id='run-8f86da03-5e15-41f7-ba58-8e968b79b16b-0', usage_metadata={'input_tokens': 12, 'output_tokens': 19, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=llm_name)\n",
    "\n",
    "# Define a node that uses the LLM to respond\n",
    "def chat_node(state):\n",
    "    # Get messages from state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Add response to messages\n",
    "    new_messages = messages + [response]\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\"messages\": new_messages}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add the chat node\n",
    "workflow.add_node(\"chat\", chat_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"chat\")\n",
    "\n",
    "# Add edge to end\n",
    "workflow.add_edge(\"chat\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run the graph with an initial message\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Tell me a short joke\")]\n",
    "})\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='Tell me a short joke', additional_kwargs={}, response_metadata={}, id='8b2e6ca3-4d95-4d8f-a6f0-1b2aaeda5691')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 12, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'stop', 'logprobs': None}, id='run-0b75ace4-aca6-4fa2-9b50-e8081d8086e3-0', usage_metadata={'input_tokens': 12, 'output_tokens': 19, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langgraph",
   "language": "python",
   "name": "oreilly-langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
