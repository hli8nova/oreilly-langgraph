{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining Pattern for Quiz Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this [article by Anthropic](https://www.anthropic.com/research/building-effective-agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "#load env variables from .env file\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(openai_key == None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "llm_structured = ChatOllama(model=\"llama3.2\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3519: UserWarning: WARNING! response_format is not default parameter.\n",
      "                response_format was transferred to model_kwargs.\n",
      "                Please confirm that response_format is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm_name = \"gpt-4o-mini\"\n",
    "\n",
    "llm_structured = ChatOpenAI(model=llm_name, api_key=openai_key, response_format=\"json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a reviewer that scores the quality of quizzes based on input content.\n",
      "    Consider SOLELY this input below as the source: \n",
      " '''Persistence in LangGraph means keeping state information of the graph throughout its execution..'''\n",
      "    Now, analyse this quiz created based on the input source:\n",
      " '''1. What is life?'''. \n",
      "    Review this quiz and ONLY return:\n",
      "    - 'APPROVED' if the quiz is good enough, relevant and it covers well all the important contents of the original material. \n",
      "    - 'TO-REVIEW' if the quiz is not relevant to the original material showed above, or not comprehensive enough.\n",
      "    \n"
     ]
    },
    {
     "ename": "ConnectError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_transports/default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpcore/_backends/sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    205\u001b[0m }\n\u001b[0;32m--> 207\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m review_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a reviewer that scores the quality of quizzes based on input content.\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    Consider SOLELY this input below as the source: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    Now, analyse this quiz created based on the input source:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_quiz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    - \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTO-REVIEW\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if the quiz is not relevant to the original material showed above, or not comprehensive enough.\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(review_prompt)\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mllm_reviewer_structured\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m output\u001b[38;5;241m.\u001b[39mquiz_score\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/langchain_ollama/chat_models.py:517\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/ollama/_client.py:236\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    234\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/ollama/_client.py:99\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     96\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/ollama/_client.py:70\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_transports/default.py:235\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[0;32m--> 235\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-langgraph/lib/python3.11/site-packages/httpx/_transports/default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     88\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ReviewOutput(BaseModel):\n",
    "    quiz_score: str = Field(description=\"'APPROVED' or 'TO-REVIEW'\")\n",
    "\n",
    "llm_reviewer_structured = llm.with_structured_output(ReviewOutput)\n",
    "\n",
    "input_source = \"Persistence in LangGraph means keeping state information of the graph throughout its execution.\"\n",
    "initial_quiz = \"1. What is life?\"\n",
    "\n",
    "review_prompt = f\"\"\"You are a reviewer that scores the quality of quizzes based on input content.\n",
    "    Consider SOLELY this input below as the source: \\n '''{input_source}.'''\n",
    "    Now, analyse this quiz created based on the input source:\\n '''{initial_quiz}'''. \n",
    "    Review this quiz and ONLY return:\n",
    "    - 'APPROVED' if the quiz is good enough, relevant and it covers well all the important contents of the original material. \n",
    "    - 'TO-REVIEW' if the quiz is not relevant to the original material showed above, or not comprehensive enough.\n",
    "    \"\"\"\n",
    "    \n",
    "print(review_prompt)\n",
    "output = llm_reviewer_structured.invoke(review_prompt)\n",
    "output.quiz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class QuizState(TypedDict):\n",
    "    input_source: str\n",
    "    n_questions: str\n",
    "    quiz: str\n",
    "    quiz_quality_score: str\n",
    "    improved_quiz: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "def create_quiz(state: QuizState) -> QuizState:\n",
    "    n_questions = state[\"n_questions\"]\n",
    "    input_source = state[\"input_source\"]\n",
    "    quiz = llm.invoke(f\"Create a markdown styled quiz with {n_questions} given this content:\\n {input_source}\")\n",
    "    return {\"quiz\": quiz.content}\n",
    "\n",
    "def review_quiz(state: QuizState) -> QuizState:\n",
    "    input_source = state[\"input_source\"]\n",
    "    initial_quiz = state[\"quiz\"]\n",
    "    review_prompt = f\"\"\"You are a reviewer that scores the quality of quizzes based on input content.\n",
    "    Consider SOLELY this input below as the source: \\n '''{input_source}.'''\n",
    "    Now, analyse this quiz created based on the input source:\\n '''{initial_quiz}'''. \n",
    "    Review this quiz and ONLY return:\n",
    "    - 'APPROVED' if the quiz is good enough, relevant, has the right number of questions given the input and it\n",
    "    covers well all the important contents of the original material. \n",
    "    - 'TO-REVIEW' if the quiz is not relevant to the original material showed above, or not comprehensive enough.\n",
    "    \"\"\"\n",
    "    \n",
    "    quiz_quality_score_output = llm_reviewer_structured.invoke(review_prompt)\n",
    "    quiz_quality_score = quiz_quality_score_output.quiz_score\n",
    "    \n",
    "    return {\"quiz_quality_score\": quiz_quality_score}\n",
    "\n",
    "def route_quiz_feedback(state: QuizState) -> QuizState:\n",
    "    if state[\"quiz_quality_score\"]==\"APPROVED\":\n",
    "        return \"approved\"\n",
    "    elif state[\"quiz_quality_score\"]==\"TO-REVIEW\":\n",
    "        return \"improve\"\n",
    "    \n",
    "\n",
    "def write_improved_quiz(state: QuizState) -> QuizState:\n",
    "    \n",
    "    input_source = state[\"input_source\"]\n",
    "    initial_quiz = state[\"initial_quiz\"]\n",
    "    \n",
    "    prompt_improve_quiz = f\"\"\"This input was given as the ONLY source: \\n\\n '''{input_source}'''.\n",
    "    This is the first version of a quiz based only on this source: '''{initial_quiz}'''.\n",
    "    Write an improved version of this quiz by:\n",
    "    \n",
    "    1. Consider 3 points of improvement\n",
    "    2. Write the improved version of the quiz integrating the feedback\n",
    "    3. OUTPUT ONLY THE IMPROVED QUIZ AS A NUMBERED LIST.\"\"\"\n",
    "    \n",
    "    improved_quiz = llm.invoke(prompt_improve_quiz)\n",
    "    \n",
    "    return {\"improved_quiz\": improved_quiz.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(QuizState)\n",
    "\n",
    "workflow.add_node(\"create_quiz\", create_quiz)\n",
    "workflow.add_node(\"review_quiz\", review_quiz)\n",
    "workflow.add_node(\"improve_quiz\", write_improved_quiz)\n",
    "\n",
    "workflow.add_edge(START, \"create_quiz\")\n",
    "workflow.add_edge(\"create_quiz\", \"review_quiz\")\n",
    "workflow.add_conditional_edges(\"review_quiz\", route_quiz_feedback, {\"approved\": END, \"improve\": \"improve_quiz\"})\n",
    "workflow.add_edge(\"improve_quiz\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Image\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_source': '\\n1. Persistence in LangGraph means keeping state information of the graph throughout its execution\\n2. In LangGraph there are 2 fundamental questions to ask: 1. Which variables to track across the graph\\'s execution 2. Which intermediate artifacts are useful for debugging?\\n3. Sub-graphs in langgraph are graphs used as nodes in other graphs (need to share at least 1 key in the state schemas in order to communicate between sub graph and parent graph\\n4. Command in Langgraph is a node that works to update state and route to other nodes\\n5. Arguments for why a framework like langgraph can be useful:\\n    * Implementing foundational agentic patterns does not require a framework like LangGraph.\\n    * LangGraph aims to minimize overhead of implementing these patterns.\\n    * LangGraph provides supporting infrastructure underneath any workflow/agent.\\n\\n    Additionally, here is a breakdown of the specific features mentioned:\\n\\n    * Persistence:\\n    + Memory Area\\n    + Fullscreen Window\\n    + Scrolling\\n    + Timer\\n    + Aa (assuming \"Aa\" refers to some sort of annotation or feedback mechanism)\\n    + Recording\\n    * Human-In-The-Loop:\\n    + Allows for human intervention and oversight in the workflow/agent\\n    * Streaming:\\n    + Enables real-time processing of LLM calls or steps in a workflow/agent\\n',\n",
       " 'n_questions': 5,\n",
       " 'quiz': \"**LangGraph Fundamentals Quiz**\\n=====================================\\n\\nTest your knowledge about LangGraph, a framework for implementing agentic patterns.\\n\\n### Question 1: What does persistence in LangGraph mean?\\n\\nWhat does persistence in LangGraph refer to?\\n| Answer | Description |\\n| --- | --- |\\n| A) Memory Management | Incorrect |\\n| B) Keeping state information of the graph throughout its execution | Correct |\\n| C) Human Intervention | Incorrect |\\n\\n### Question 2: Which fundamental questions should you ask when designing a LangGraph?\\n\\nWhich fundamental questions should you ask when designing a LangGraph?\\n| Answer | Description |\\n| --- | --- |\\n| A) What are the required variables to track across the graph's execution? | Correct |\\n| B) How many intermediate artifacts are useful for debugging? | Correct |\\n| C) Can I use arbitrary nodes in my graph? | Incorrect |\\n\\n### Question 3: What type of nodes can be used as sub-graphs in LangGraph?\\n\\nWhat type of nodes can be used as sub-graphs in LangGraph?\\n| Answer | Description |\\n| --- | --- |\\n| A) Nodes that share a common key in the state schemas with the parent graph | Correct |\\n| B) Any type of node | Incorrect |\\n\\n### Question 4: What is the primary function of a Command node in LangGraph?\\n\\nWhat is the primary function of a Command node in LangGraph?\\n| Answer | Description |\\n| --- | --- |\\n| A) To update state and route to other nodes | Correct |\\n| B) To store data in memory | Incorrect |\\n\\n### Question 5: Why can a framework like LangGraph be useful for implementing agentic patterns?\\n\\nWhy can a framework like LangGraph be useful for implementing agentic patterns?\\n| Answer | Description |\\n| --- | --- |\\n| A) It reduces the complexity of implementing agentic patterns | Correct |\\n| B) It eliminates the need for a custom framework | Incorrect |\\n| C) It provides supporting infrastructure underneath any workflow/agent | Correct |\\n\\n**Additional Features**\\n\\n### Persistence\\n\\nWhich features are included in persistence?\\n| Answer | Description |\\n| --- | --- |\\n| A) Memory Area | Correct |\\n| B) Fullscreen Window | Correct |\\n| C) Scrolling | Correct |\\n| D) Timer | Correct |\\n| E) Aa (annotation or feedback mechanism) | Correct |\\n| F) Recording | Correct |\\n\\n### Human-In-The-Loop\\n\\nWhat type of human intervention is supported by LangGraph?\\n| Answer | Description |\\n| --- | --- |\\n| A) Allows for human intervention and oversight in the workflow/agent | Correct |\\n| B) Requires human intervention to execute tasks | Incorrect |\\n\\n### Streaming\\n\\nWhich feature enables real-time processing of LLM calls or steps in a workflow/agent?\\n| Answer | Description |\\n| --- | --- |\\n| A) Enables streaming | Correct |\\n| B) Disables streaming | Incorrect |\\n\\nLet me know if you need any further assistance!\",\n",
       " 'quiz_quality_score': 'APPROVED'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_source_raw = \"\"\"\n",
    "1. Persistence in LangGraph means keeping state information of the graph throughout its execution\n",
    "2. In LangGraph there are 2 fundamental questions to ask: 1. Which variables to track across the graph's execution 2. Which intermediate artifacts are useful for debugging?\n",
    "3. Sub-graphs in langgraph are graphs used as nodes in other graphs (need to share at least 1 key in the state schemas in order to communicate between sub graph and parent graph\n",
    "4. Command in Langgraph is a node that works to update state and route to other nodes\n",
    "5. Arguments for why a framework like langgraph can be useful:\n",
    "    * Implementing foundational agentic patterns does not require a framework like LangGraph.\n",
    "    * LangGraph aims to minimize overhead of implementing these patterns.\n",
    "    * LangGraph provides supporting infrastructure underneath any workflow/agent.\n",
    "\n",
    "    Additionally, here is a breakdown of the specific features mentioned:\n",
    "\n",
    "    * Persistence:\n",
    "    + Memory Area\n",
    "    + Fullscreen Window\n",
    "    + Scrolling\n",
    "    + Timer\n",
    "    + Aa (assuming \"Aa\" refers to some sort of annotation or feedback mechanism)\n",
    "    + Recording\n",
    "    * Human-In-The-Loop:\n",
    "    + Allows for human intervention and oversight in the workflow/agent\n",
    "    * Streaming:\n",
    "    + Enables real-time processing of LLM calls or steps in a workflow/agent\n",
    "\"\"\"\n",
    "\n",
    "output = graph.invoke({\"input_source\": input_source_raw,\n",
    "                       \"n_questions\": 5})\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**LangGraph Fundamentals Quiz**\n",
       "=====================================\n",
       "\n",
       "Test your knowledge about LangGraph, a framework for implementing agentic patterns.\n",
       "\n",
       "### Question 1: What does persistence in LangGraph mean?\n",
       "\n",
       "What does persistence in LangGraph refer to?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) Memory Management | Incorrect |\n",
       "| B) Keeping state information of the graph throughout its execution | Correct |\n",
       "| C) Human Intervention | Incorrect |\n",
       "\n",
       "### Question 2: Which fundamental questions should you ask when designing a LangGraph?\n",
       "\n",
       "Which fundamental questions should you ask when designing a LangGraph?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) What are the required variables to track across the graph's execution? | Correct |\n",
       "| B) How many intermediate artifacts are useful for debugging? | Correct |\n",
       "| C) Can I use arbitrary nodes in my graph? | Incorrect |\n",
       "\n",
       "### Question 3: What type of nodes can be used as sub-graphs in LangGraph?\n",
       "\n",
       "What type of nodes can be used as sub-graphs in LangGraph?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) Nodes that share a common key in the state schemas with the parent graph | Correct |\n",
       "| B) Any type of node | Incorrect |\n",
       "\n",
       "### Question 4: What is the primary function of a Command node in LangGraph?\n",
       "\n",
       "What is the primary function of a Command node in LangGraph?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) To update state and route to other nodes | Correct |\n",
       "| B) To store data in memory | Incorrect |\n",
       "\n",
       "### Question 5: Why can a framework like LangGraph be useful for implementing agentic patterns?\n",
       "\n",
       "Why can a framework like LangGraph be useful for implementing agentic patterns?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) It reduces the complexity of implementing agentic patterns | Correct |\n",
       "| B) It eliminates the need for a custom framework | Incorrect |\n",
       "| C) It provides supporting infrastructure underneath any workflow/agent | Correct |\n",
       "\n",
       "**Additional Features**\n",
       "\n",
       "### Persistence\n",
       "\n",
       "Which features are included in persistence?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) Memory Area | Correct |\n",
       "| B) Fullscreen Window | Correct |\n",
       "| C) Scrolling | Correct |\n",
       "| D) Timer | Correct |\n",
       "| E) Aa (annotation or feedback mechanism) | Correct |\n",
       "| F) Recording | Correct |\n",
       "\n",
       "### Human-In-The-Loop\n",
       "\n",
       "What type of human intervention is supported by LangGraph?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) Allows for human intervention and oversight in the workflow/agent | Correct |\n",
       "| B) Requires human intervention to execute tasks | Incorrect |\n",
       "\n",
       "### Streaming\n",
       "\n",
       "Which feature enables real-time processing of LLM calls or steps in a workflow/agent?\n",
       "| Answer | Description |\n",
       "| --- | --- |\n",
       "| A) Enables streaming | Correct |\n",
       "| B) Disables streaming | Incorrect |\n",
       "\n",
       "Let me know if you need any further assistance!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(output[\"quiz\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Pattern with LangGraph\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import display, Image\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainationState(TypedDict):\n",
    "    question: str\n",
    "    analogy: str\n",
    "    examples: str\n",
    "    plain_english: str\n",
    "    technical_definition: str\n",
    "    full_explanation: str\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "def generate_analogy(state: ExplainationState)->ExplainationState:\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    analogy_prompt = f\"\"\"\n",
    "    Given this question by the user:\n",
    "    {question}\n",
    "    Explain it by providing an analogy to help.\n",
    "    \"\"\"\n",
    "    analogy_output = llm.invoke(analogy_prompt)\n",
    "    return {\"analogy\":  analogy_output.content}\n",
    "\n",
    "def generate_example(state: ExplainationState)->ExplainationState:\n",
    "    question = state[\"question\"]\n",
    "    example_prompt = f\"\"\"Given this question by the user: \n",
    "    {question}\n",
    "    Provide 3 examples to help clarify it or demonstrate it in context.\n",
    "    Your OUTPUT SHOULD ONLY BE the examples as a bullet point list.\"\"\"\n",
    "    \n",
    "    examples_output = llm.invoke(example_prompt)\n",
    "    \n",
    "    return {\"examples\": examples_output.content}\n",
    "\n",
    "def explain_plain_english(state: ExplainationState)->ExplainationState:\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    plain_english_prompt = f\"\"\"\n",
    "    Given this question by the user:\n",
    "    {question}\n",
    "    Explain it in simple plain english terms in less than 2 paragraphs.\n",
    "    Output plain english explanation:\n",
    "    \"\"\"\n",
    "    output_plain_english = llm.invoke(plain_english_prompt)\n",
    "\n",
    "    return {\"plain_english\":output_plain_english.content}\n",
    "\n",
    "def explain_technical_definition(state: ExplainationState)->ExplainationState:\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    technical_definition_prompt = f\"\"\"\n",
    "    Given this question by the user:\n",
    "    {question}\n",
    "    Explain it in technical terms not using more than 2 paragraphs. Make sure to be rigid and\n",
    "    give the official technical definition required that answers the question.\n",
    "    Output technical definition:\n",
    "    \"\"\"\n",
    "    technical_definition_output = llm.invoke(technical_definition_prompt)\n",
    "    return {\"technical_definition\": technical_definition_output.content}\n",
    "\n",
    "def generate_final_explanation(state: ExplainationState)->ExplainationState:\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    analogy = state[\"analogy\"]\n",
    "    examples = state[\"examples\"]\n",
    "    plain_english = state[\"plain_english\"]\n",
    "    technical_definition = state[\"technical_definition\"]\n",
    "    \n",
    "    full_explanation = f\"\"\"\n",
    "    # Final answer\n",
    "    \n",
    "    ## Analogy\n",
    "    \n",
    "    {analogy}\n",
    "    \n",
    "    ## Examples\n",
    "    \n",
    "    {examples}\n",
    "    \n",
    "    ## Plain English\n",
    "    \n",
    "    {plain_english}\n",
    "    \n",
    "    ## Technical Definition\n",
    "    \n",
    "    {technical_definition}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return {\"full_explanation\": full_explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ExplainationState)\n",
    "\n",
    "workflow.add_node(\"generate_analogy\",generate_analogy)\n",
    "workflow.add_node(\"generate_example\",generate_example)\n",
    "workflow.add_node(\"explain_plain_english\",explain_plain_english)\n",
    "workflow.add_node(\"explain_technical_definition\",explain_technical_definition)\n",
    "workflow.add_node(\"generate_final_explanation\",generate_final_explanation)\n",
    "\n",
    "workflow.add_edge(START, \"generate_analogy\")\n",
    "workflow.add_edge(START, \"generate_example\")\n",
    "workflow.add_edge(START, \"explain_plain_english\")\n",
    "workflow.add_edge(START,\"explain_technical_definition\")\n",
    "\n",
    "workflow.add_edge(\"generate_analogy\",\"generate_final_explanation\")\n",
    "workflow.add_edge(\"generate_example\",\"generate_final_explanation\")\n",
    "workflow.add_edge(\"explain_plain_english\",\"generate_final_explanation\")\n",
    "workflow.add_edge(\"explain_technical_definition\",\"generate_final_explanation\")\n",
    "workflow.add_edge(\"generate_final_explanation\",END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "graph.invoke({\"question\": \"Explain the concept of self-attention in transformers (context is large language models).\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "Let's look at a simple example of routing between LLMs. We'll create a workflow that:\n",
    "\n",
    "1. Takes a user question\n",
    "2. Routes it to one of three specialized agents:\n",
    "   - Code explanation agent\n",
    "   - Math problem solver\n",
    "   - General knowledge agent\n",
    "3. Gets the specialized response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will be using an LLM with [structured output](https://platform.openai.com/docs/guides/structured-outputs) capabilities\n",
    "in order to obtain an output from the LLM in a controllable format (json) which we can then extract a key that will\n",
    "be used to route a question to one of 3 different LLMs (containing different prompts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define our state\n",
    "class RoutingState(TypedDict):\n",
    "    question: str  # Input question\n",
    "    category: str | None # Routing classification\n",
    "    answer: str | None  # Final answer\n",
    "\n",
    "class QuestionType(BaseModel):\n",
    "    category: str = Field(description=\"The category classification of the question: CODE or MATH or GENERAL\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_structured_output = llm.with_structured_output(QuestionType)\n",
    "\n",
    "# Router that classifies the question\n",
    "def route_question(state: RoutingState):\n",
    "    question = state[\"question\"]\n",
    "    response = llm_with_structured_output.invoke(\n",
    "        f\"\"\"Classify this question into exactly ONE category:\n",
    "        - CODE if about programming/coding\n",
    "        - MATH if about mathematical calculations\n",
    "        - GENERAL for general knowledge\n",
    "        Question: {question}\n",
    "        Category:\"\"\"\n",
    "    )\n",
    "    print(response)\n",
    "    return {\"category\": response.category}\n",
    "\n",
    "# Specialized agents\n",
    "def code_expert(state: RoutingState):\n",
    "    print(\"Using code expert!\")\n",
    "    return {\"answer\": llm.invoke(f\"As a coding expert, answer: {state['question']}\")}\n",
    "\n",
    "def math_expert(state: RoutingState):\n",
    "    print(\"Using math expert\")\n",
    "    return {\"answer\": llm.invoke(f\"As a math expert, solve: {state['question']}\")}\n",
    "\n",
    "def general_expert(state: RoutingState):\n",
    "    print(\"Using general expert\")\n",
    "    return {\"answer\": llm.invoke(f\"Answer this general question: {state['question']}\")}\n",
    "\n",
    "# Define routing logic\n",
    "def router(state: RoutingState):\n",
    "    if state[\"category\"] == \"CODE\":\n",
    "        return \"code_expert\"\n",
    "    elif state[\"category\"] == \"MATH\":\n",
    "        return \"math_expert\"\n",
    "    else:\n",
    "        return \"general_expert\"\n",
    "\n",
    "# Create and configure workflow\n",
    "workflow = StateGraph(RoutingState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"route\", route_question)\n",
    "workflow.add_node(\"code_expert\", code_expert)\n",
    "workflow.add_node(\"math_expert\", math_expert) \n",
    "workflow.add_node(\"general_expert\", general_expert)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"route\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"route\",\n",
    "    router,\n",
    "    {\n",
    "        \"code_expert\": \"code_expert\",\n",
    "        \"math_expert\": \"math_expert\",\n",
    "        \"general_expert\": \"general_expert\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"code_expert\", END)\n",
    "workflow.add_edge(\"math_expert\", END)\n",
    "workflow.add_edge(\"general_expert\", END)\n",
    "\n",
    "# Compile and run\n",
    "graph = workflow.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='CODE'\n",
      "Using code expert!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the time complexity of quicksort?',\n",
       " 'category': 'CODE',\n",
       " 'answer': AIMessage(content='The time complexity of the quicksort algorithm can vary depending on the circumstances:\\n\\n1. **Average Case:** The average time complexity of quicksort is \\\\(O(n \\\\log n)\\\\), where \\\\(n\\\\) is the number of elements in the array. This is because the partitioning process divides the array into two parts, and the recursion depth is approximately \\\\(\\\\log n\\\\), with each level of recursion requiring \\\\(O(n)\\\\) work to partition the array.\\n\\n2. **Best Case:** The best-case time complexity is also \\\\(O(n \\\\log n)\\\\). This occurs when the pivot selection results in equally balanced partitions at each step, leading to a perfectly balanced recursive tree.\\n\\n3. **Worst Case:** The worst-case time complexity is \\\\(O(n^2)\\\\). This occurs when the pivot selection results in highly unbalanced partitions, such as when the smallest or largest element is consistently chosen as the pivot. An example of this situation is when the array is already sorted (or reverse sorted) and the first or last element is chosen as the pivot.\\n\\nTo mitigate the worst-case scenario, techniques such as using a random pivot or the \"median-of-three\" method for pivot selection are often employed, which help ensure that quicksort operates closer to its average-case complexity in practice.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 260, 'prompt_tokens': 23, 'total_tokens': 283, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-675a3b9d-a5aa-40b0-8191-05f74f6fbe68-0', usage_metadata={'input_tokens': 23, 'output_tokens': 260, 'total_tokens': 283, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\n",
    "     \"question\": \"What is the time complexity of quicksort?\",   # Routes to code expert\n",
    "     \"category\": None,\n",
    "     \"answer\": None\n",
    " })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langgraph",
   "language": "python",
   "name": "oreilly-langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
